DataPY V1.1 – Living Specification / Handoff Sheet 
Last updated: 2025 07 12
1 Business Goal & Scope (v1)
Deliver a CLI first, YAML driven ETL framework—"Talend like" behaviour but 100 % Python and cloud agnostic—suitable for nightly batch jobs today and scalable to distributed clusters tomorrow.
In scope for v1
•	CLI commands: build · run · plan · create · pack
•	YAML job file with sections job, job_config, components, connections
•	Data edges + Control edges (ok, error, if, subjob_ok, subjob_error) with implicit parallelization via multiple control edges
forEach iteration component with nested iteration support and iterator-aware subjob boundaries
•	Context variable substitution + component global variable substitution
•	Execution mode and configurations are job level see the Json struct below
•	SQLite component registry (schema locked)
•	Build artefact .pjob (ZIP with manifest.json envelope, original YAML, dag.msgpack)
•	Secrets via CyberArk only (env vars disallowed)
•	Idempotency flags, retry/timeout, cascade skip semantics
•	Structured JSON logging; metrics exporter interface, in dept logging during enginge execution
•	Per-run checkpoint store with subjob boundary resume capability
•	Immutable global variables contract with thread-safe updates
Iterator-aware BufferedStore for efficient global variable management within iterations
•	≤200 LOC per file, pluggable architecture, LOC excludes comments and blank lines
Out of scope v1 (parking lot)
scheduling, remote checkpoints (S3/GCS), advanced schema versioning, GUI, secret rotation.“Component-level retries, OR control-edge support and dead-branch detection are out-of-scope for v1 but are explicitly tracked in the post-v1 roadmap.”
Critical v1 Production Requirements
•	Subjob Resume Logic: Checkpointing must properly handle subjob boundaries and allow resume from last successful subjob
Iterator Subjob Management: forEach components force all downstream components into same subjob for atomic iteration processing
•	Flexible Data Contract: data can either be pandas or dask baed on execution mode, but the main data struct is {str:Any}(Publish two TypedDicts (one for pandas, one for dask) and union-type them. Components still receive a single argument, but type-checkers now guard column names & dtypes.)the str contains the to and from address and Any is either pandas or dask based on execution mode
•	Connection Management: Built-in connection pooling and retry logic for database/API components
•	Robust Engine that ensures data safety and completion, can trade performance for this if needed

1a Sample YAML Structure – Specification
{
  "type": "object",
  "required": ["job", "job_config", "components", "connections"],
  "additionalProperties": false,
  "properties": {
    "job": {
      "type": "object",
      "required": ["name", "version", "team", "owner", "created"],
      "additionalProperties": false,
      "properties": {
        "name": {
          "type": "string",
          "pattern": "^[a-zA-Z][a-zA-Z0-9_-]*$",
          "maxLength": 128
        },
        "desc": {
          "type": "string",
          "maxLength": 512
        },
        "version": {
          "type": "string",
          "pattern": "^\\d+\\.\\d+\\.\\d+$"
        },
        "team": {
          "type": "string",
          "pattern": "^[a-zA-Z][a-zA-Z0-9_-]*$"
        },
        "owner": {
          "type": "string",
          "format": "email"
        },
        "created": {
          "type": "string",
          "pattern": "^\\d{4}-\\d{2}-\\d{2}$"
        }
      }
    },
    "job_config": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "retries": {
          "type": "integer",
          "minimum": 0,
          "maximum": 3,
          "default": 1
        },
        "timeout": {
          "type": "integer",
          "minimum": 1,
          "default": 3600
        },
        "fail_strategy": {
          "type": "string",
          "enum": ["halt", "continue"],
          "default": "halt"
        },
        "execution_mode": {
          "type": "string",
          "enum": ["pandas", "dask"],
          "default": "pandas",
          "description": "Job-level execution mode for all components"
        },
        "chunk_size": {
          "type": "string",
          "pattern": "^\\d+(\\.\\d+)?[KMGT]?B$",
          "default": "200MB",
          "description": "Data chunk size for dask mode processing"
        },
        "execution": {
          "type": "object",
          "additionalProperties": false,
          "properties": {
            "threadpool": {
              "type": "object",
              "additionalProperties": false,
              "properties": {
                "max_workers": {
                  "type": "integer",
                  "minimum": 1,
                  "maximum": 128,
                  "default": 8,
                  "description": "Maximum threadpool workers for pandas components"
                }
              }
            },
            "dask": {
              "type": "object",
              "additionalProperties": false,
              "properties": {
                "cluster_workers": {
                  "type": "integer",
                  "minimum": 1,
                  "maximum": 1024,
                  "description": "Total number of workers in the dask cluster"
                },
                "threads_per_worker": {
                  "type": "integer",
                  "minimum": 1,
                  "maximum": 32,
                  "default": 2,
                  "description": "Threads per dask worker"
                },
                "memory_per_worker": {
                  "type": "string",
                  "pattern": "^\\d+(\\.\\d+)?[KMGT]?B$",
                  "default": "4GB",
                  "description": "Default memory allocation per dask worker"
                }
              }
            }
          }
        }
      }
    },
    "components": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["name", "type", "params"],
        "additionalProperties": false,
        "properties": {
          "name": {
            "type": "string",
            "pattern": "^[a-zA-Z][a-zA-Z0-9_]*$",
            "maxLength": 64
          },
          "type": {
            "type": "string",
            "pattern": "^[a-zA-Z][a-zA-Z0-9_]*$"
          },
          "params": {
            "type": "object",
            "additionalProperties": true
          }
        }
      }
    },
    "connections": {
      "type": "object",
      "required": ["data", "control"],
      "additionalProperties": false,
      "properties": {
        "data": {
          "type": "array",
          "items": {
            "type": "string",
            "pattern": "^[a-zA-Z][a-zA-Z0-9_]*\\.[a-zA-Z][a-zA-Z0-9*_]*\\s*->\\s*[a-zA-Z][a-zA-Z0-9_]*\\.[a-zA-Z][a-zA-Z0-9*_]*$"
          }
        },
        "control": {
          "type": "array",
          "items": {
            "type": "string",
            "anyOf": [
              {
                "pattern": "^[a-zA-Z][a-zA-Z0-9_]*\\s*\\(if\\d+\\)\\s*:\\s*\"[^\"]*\"\\s+[a-zA-Z][a-zA-Z0-9_]*$",
                "description": "Conditional control edge: component (if1): \"condition\" target"
              },
              {
                "pattern": "^[a-zA-Z][a-zA-Z0-9_]*\\s*\\((ok|error|subjob_ok|subjob_error)\\)\\s+[a-zA-Z][a-zA-Z0-9_]*$",
                "description": "Standard control edge: component (trigger) target"
              }
            ]
          }
        }
      }
    }
  }
}
Notes:
•	{{context.VAR}} resolved at compile-time for environment configuration
•	compName__GLOBAL_VAR resolved at runtime, after the component emits the global


2 Core Packages and Ownership
Package	Purpose
pype.core.engine.*	Orchestrator, execution manager, multi-executor routing
pype.core.loader.*	YAML→JobModel, templating, schema validation, CyberArk injection
pype.core.planner.*	DAG build, sub job detection, validation
pype.core.registry.*	SQLite catalogue of components
pype.core.observability.*	JSON logs, optional metrics, event hooks
pype.core.checkpointing.*	Per-run checkpoint store with subjob boundary support
pype.core.utils.secrets	CyberArk provider implementation
pype.cli.*	All user facing commands
pype.components.*	BaseComponent + standard IO/Transform/Control nodes


3 Dependency Baseline
The following runtime libraries are approved for v1:
Purpose	Library	Rationale
YAML streaming	ruamel.yaml	Mature, >40 M downloads/month
Data classes & validation	pydantic	de facto standard in Python API projects
CLI parsing	click	Stable, 10 yrs old
DAG utilities	networkx	Widely cited academic & industrial graph lib
Parallel data	dask[complete]	Broad adoption in data engineering
Binary serialisation	msgpack	Small, fast, std like
HTTP client	requests	Used by custom CyberArk REST/CLI adapter
Tabular data frame	pandas	Core in memory engine for small jobs
Schema validation	jsonschema	Used in Loader validator


## 4 File/Folder Layout (v1) - Simplified

```
pyproject.toml  # dependency & metadata root file
pype/
├─ cli/
│   ├─ cli.py            
│   ├─ build.py           
│   ├─ run.py             
│   ├─ compoennt_creator.py  
│   ├─ dag_visualize.py   
│   ├─ inspect_pjob.py 
│   ├─ setup.py
│   └─ registry.py        
├─ core/
│   ├─ engine/
│   │   ├─ orchestrator.py        # High-level job coordination & flow control
│   │   ├─ globals_store.py  
│   │   └─ execution_manager.py   # Simplified executor routing (pandas/dask choice)
│   ├─ loader/
│   │   ├─ templater.py
│   │   ├─ validator.py
│   │   └─ loader.py
│   ├─ planner/
│   │   ├─ graph_builder.py
│   │   ├─ port_resolver.py
│   │   ├─ subjob_analyzer.py
│   │   ├─ structure_validator.py
│   │   └─ planner.py
│   ├─ registry/
│   │   ├─ componets_registry.py
│   │   ├─ joblet_registry.py
│   │   └─ sqlite_backend.py
│   ├─ observability/
│   │   ├─ logging.py
│   │   ├─ metrics.py
│   │   └─ hooks.py
│   ├─ checkpointing/
│   │   ├─ checkpoint_mgr.py
│   │   └─ run_store.py
│   └─ utils/
│       ├─ constants.py
│       └─ secrets.py
├─ components/
│   ├─ base.py
│   ├─ sink/
│   ├─ source/
│   ├─ quality/
│   ├─ utility/
│   ├─ transform/
│   └─ control/
└─ schemas/
    ├─ job.schema.json
```

### Engine Module Requirement:
Process the dag inside the .pjob build package
Must handle and route data and control connections
Should be aware of subjob and iterator boundaries and should know how to process them
SHould use asyncio and threadpooling from subjob management, parallelism etc. (important: topological sort to find the order first SHOULD BE DOEN ARE pLANNING phase)
properly implement all the fetures in the job_config section of the json schema
Robust Engine that ensures data safety and completion, can trade performance for this if needed

Concurrency guard important – “The orchestrator MUST ensure that any blocking pandas/Disk-I/O code is executed via await loop.run_in_executor(pool, …) so the asyncio event-loop is never stalled.”

Retry scope – “v1 implements retries only at sub-job granularity; individual components inherit the sub-job attempt and never retry on their own.”


## 5 Control‑Flow Semantics (Simplified Model)

### Control Edge Types:
- **ok / error** – fire after component success/failure
- **if (order:int:condition)** – conditional control link
- **subjob_ok / subjob_error** – emitted when a sub‑job finishes
ok  – emitted when forEach component completes all iterations

### Subjob Boundary Rules:
- Control edges create subjob boundaries
- forEach Exception: All components downstream of forEach components are forced into the same subjob as the forEach component
- Restricted Control Edges: Components downstream of forEach cannot emit subjob_ok or subjob_error edges
- Iterator Completion: Only ok  edges from forEach components can create new subjob boundaries
- Data edges connect within subjobs  
- Implicit parallelization via multiple control edges from same component
- Components wait for ALL incoming control&data edges before starting (default AND logic)

### Execution Control:
- Job-level execution mode choice (pandas or dask)
- Components adapt to job execution mode automatically
- Disk-based lookup components handle internal conversions with needed values set in optional params

### Data Handling Philosophy:
- Engine passes raw DataFrames between components (no wrapper) {str:ANy} str froma nd tro address, Any actual data- pd or dask
- Components receive pandas or dask DataFrames based on job execution mode
- Zero conversion overhead within same execution mode
- Lookup components handle disk operations and format conversion internally

forEach Component Semantics:

Input: DataFrame with N rows to iterate over
Output: DataFrame with 1 row per iteration (current item)
Iteration Control: Component controls termination via data exhaustion or explicit completion
Nested Support: Inner forEach components execute fully before outer forEach advances
Error Handling: Errors within forEach scope handled per iteration; failures don't terminate entire forEach cycle








## 6 Build Artefact (.pjob) - Updated Format

ZIP container with envelope format:

- **manifest.json** – envelope metadata: {"format":"datapy-pjob@1", "yaml":"job_original.yaml", "dag_msgpack":"dag.msgpack", "manifest":{...}}
- **job_original_name.yaml** – untouched source for audit
- **dag.msgpack** – networkx DAG + metadata
- **assets/** – lookup files, templates, etc.

## 7 Registry Table Definition (v1) - Simplified

```sql
CREATE TABLE IF NOT EXISTS components (
    name TEXT PRIMARY KEY,
    class_name TEXT NOT NULL,
    module_path TEXT NOT NULL,
    category TEXT NOT NULL DEFAULT 'unknown',
    description TEXT,
    input_ports TEXT NOT NULL DEFAULT '[]',
    output_ports TEXT NOT NULL DEFAULT '[]',
    required_params TEXT NOT NULL DEFAULT '{}',
    optional_params TEXT NOT NULL DEFAULT '{}',
    output_globals TEXT NOT NULL DEFAULT '[]',
    dependencies TEXT NOT NULL DEFAULT '[]',
    startable INTEGER NOT NULL DEFAULT 0,
    allow_multi_in INTEGER NOT NULL DEFAULT 0,
    idempotent INTEGER DEFAULT 1,
    version TEXT DEFAULT '1.0.0',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## 8 Execution Architecture - Simplified Approach

### Data Size Thresholds:
```python
# Size-based execution mode decision
if data_size < 4GB:   use "pandas"   # Fast, simple, all libraries work
if data_size < 100GB: use "dask"     # Memory efficient, scales horizontally
```

### Component Implementation Pattern:
```python
class ReadCSVComponent(BaseComponent):
    def execute(self, inputs):
        execution_mode = self.context["execution_mode"]
        file_path = self.config["file_path"]
        
        # Validate file size vs execution mode
        file_size_gb = os.path.getsize(file_path) / (1024**3)
        if execution_mode == "pandas" and file_size_gb > 4:
            raise ValueError(f"File size {file_size_gb:.1f}GB too large for pandas mode")
        
        # Execute based on job execution mode
        if execution_mode == "pandas":
            return {"main": pd.read_csv(file_path)}
        elif execution_mode == "dask":
            return {"main": dd.read_csv(file_path, blocksize="200MB")}
```

### Lookup Component Pattern Example:
```python
class LookupComponent(BaseComponent):
    def execute(self, inputs):
        df = inputs["main"]  # pandas or dask based on job mode
        job_execution_mode = self.context["execution_mode"]
        
        # Handle disk-based lookup internally
        lookup_result = self._disk_lookup(df)
        
        # Return in job's expected format
        if job_execution_mode == "dask" and isinstance(lookup_result, pd.DataFrame):
            return {"main": dd.from_pandas(lookup_result, npartitions=4)}
        else:
            return {"main": lookup_result}
```

## 9 Critical Production Requirements Notes(Instead on implicit validation let the user be aware and set the config explicitly if needed) here just for dev reference

### Memory Requirements:
- **Pandas processing**: RAM = data size + 50% overhead
- **Dask processing 100GB**: ~2GB RAM peak regardless of data size
- **Disk lookup components**: Component-specific cache sizes within reasonable limits

### Future Spark Migration:
Components designed to support third execution mode:
```python
def execute(self, inputs):
    execution_mode = self.context["execution_mode"]
    if execution_mode == "spark":  # Future addition
        return self._process_spark(inputs["main"])
```

## 10 Engine Execution Strategy

### Orchestrator (Level 1):
- Asyncio-based subjob coordination
- Parallel subjob execution using `asyncio.gather`
- Checkpointing at subjob boundaries
- Control flow and error handling
- one sqlite db per job for run store etc

### ExecutionManager (Level 2):
- Routes components based on job execution mode
- Handles disk component internal conversions(or many be the component will do it?)
- Direct DataFrame passing (no wrapper overhead)

### Data Flow:
```python
# Zero overhead for same execution mode
pandas_job: pd.DataFrame → pd.DataFrame → pd.DataFrame
dask_job: dd.DataFrame → dd.DataFrame → dd.DataFrame

# Conversion only in disk components when needed
dask_job: dd.DataFrame → disk_lookup(internal conversion) → dd.DataFrame
```

## 12 Key Architectural Decisions Summary

1. **Execution Strategy**: Job-level pandas/dask choice with component adaptation
2. **Data Passing**: Direct DataFrame passing, no wrapper complexity
3. **Size Validation**: Built-in file size validation with clear error messages
4. **Disk Operations**: Component-level configuration for lookup tables only
5. **Format Consistency**: Zero conversion overhead within same execution mode
6. **Future Scaling**: Architecture ready for Spark as third execution mode
7. **Industry Alignment**: Matches Netflix/Uber pattern of tool-per-job selection






Checkpointing (v1) –
At every sub-job boundary the engine writes a per-run SQLite store capturing DAG state, global-variable snapshot and a SHA-256 hash of every external input it touched. Resume logic rejects a restart if any recorded hash drifts, guaranteeing deterministic re-runs after data changes. Old run-stores are rotated according to a configurable retention policy.

Observability (v1) –
DataPY emits structured JSON logs for every component event (start, finish, error) and exposes pluggable hooks for metrics collectors (Prometheus, StatsD, OTLP). The log schema is frozen in v1—fields include job_id, subjob_id, component, duration_ms, row_count, error_type—so downstream SIEM dashboards remain stable across upgrades.

# Globals & Payloads (v1)

• A single thread-safe **GlobalStore** exists per job-run; API: get, set(mode=replace|accumulate), revision(). Any value >64 KB is rejected.
• Edge **data** payload = {data_ref, rows, schema_hash?, checkpoint_ref?}, where data_ref is an in-memory token or URI (file/S3).
• Edge **control** payload = {trigger, edge_id, condition_passed?, global_rev}; no business data allowed.
• Components inside iterator boundaries MAY update internal counters per loop but MUST publish ONE summary dict to GlobalStore on completion (mode="replace").
• Fan-in merge: concatenate data by port; metadata uses first-non-null else error; globals are re-read from GlobalStore (never merged in payload).
• All blocking I/O must run via loop.run_in_executor to keep the asyncio event loop responsive; GlobalStore uses write-lock, lock-free reads stamped with revision().
• On every sub-job boundary, checkpoint = {GlobalStore snapshot, edge manifest}; resume validates SHA-256 of each data_ref. Older run-stores auto-rotate after 7 days (configurable).



## Global Variable Management (v1)

### 1. GlobalStore
• File: `pype/core/engine/global_store.py`  
• Scope: one instance per **job-run**; created by the engine on job start, destroyed on completion.  
• Thread-safety: protected by `threading.RLock`; reads are lock-free, writes occur inside the lock.  
• API:  
  - `get(key, default=None)`  
  - `set(key, value, *, mode="replace|accumulate")` → rejects JSON payload > 64 KB  
  - `revision()` → monotonic int  
  - `dump() -> bytes` / `from_dump(raw) -> GlobalStore` (msgpack `{rev,data}`).  
• Logging: every successful `set()` emits `GLOBAL_SET {rev,key,value,component}` to the JSON log.

### 2. BufferedStore (iterator boundaries)
• Components whose YAML metadata sets `iterator_boundary: true` receive a **BufferedStore** instead of GlobalStore.  
• BufferedStore mirrors the GlobalStore API but caches all `set()` calls in-memory during the loop.  
• At iterator completion the engine calls `BufferedStore.flush()`, writing **one** consolidated update per key to the underlying GlobalStore (mode `"replace"`).  
• Result: exactly one GlobalStore write per iterator → minimal lock contention & log volume.

### 3. Context Injection
• Component constructor: `__init__(name, config, context, global_store)`  
  - `global_store` is GlobalStore for normal components.  
  - `global_store` is BufferedStore for iterator components.  
• `context` now carries only execution data: `execution_mode`, `run_id`, `subjob_id`, `attempt`.

### 4. Fan-in & Control Edges
• Data edges carry `{data_ref, rows, schema_hash?, checkpoint_ref?}` only.  
• Control edges carry `{trigger, edge_id, condition_passed?, global_rev}`; no globals inside.  
• Downstream components must re-read needed globals after inputs converge.

### 5. Checkpoint / Resume
• On every sub-job boundary the engine persists:  
  - `globals.msgpack` ← `GlobalStore.dump()`  
  - `edge_manifest.json` ← list of `data_ref`s  
• `resume` restores via `GlobalStore.from_dump()` and verifies SHA-256 of each `data_ref`.

### 6. Concurrency Guard
• Blocking pandas / disk I-O **must** run with `await loop.run_in_executor(pool, …)` to keep the asyncio event loop responsive.  
• Keep GlobalStore lock scope micro-seconds; never wrap heavy work.

### 7. Iterator Rule of Thumb
• Inside iterator boundaries components may compute per-record metrics but MUST rely on BufferedStore so that only one summary (e.g. `<iter_name>__rows`, `<iter_name>__current`) is committed to GlobalStore at loop end.



## 13  Execution-Context Contract  (v1)

Context object = **read-only** metadata supplied by the orchestrator to every component call.
No component may mutate it; dynamic values belong in GlobalStore instead.

context = {
    "execution_mode": "pandas" | "dask",      # Job-level engine switch
    "run_id":        "<UUID>",                # Stable per job-run
    "job": {                                  # Static YAML metadata
        "name":    "<job.name>",
        "version": "<job.version>",
        "team":    "<job.team>",
        "owner":   "<job.owner>"
    },
    "subjob_id":     "<auto-id>",             # Changes at sub-job boundaries
    "attempt":       <int>,                   # 1-based retry counter
    "threadpool":    {"max_workers": <int>},  # Resolved from job_config
    "dask":          {"cluster_uri": "..."}?  # Present only in dask mode
}

• **Immutability:** Context is created by the orchestrator; Python `MappingProxyType`
  enforces read-only semantics inside components.
• **Lifecycle:** Orchestrator builds one base context at job start and clones it
  (updating `subjob_id` / `attempt`) before each component execution.
• **Maintenance:** Only the orchestrator mutates / re-issues context; components
  must never add or change keys.

### Why globals are **not** in context

| Issue if globals were copied into context | Mitigation with GlobalStore |
|-------------------------------------------|-----------------------------|
| Concurrency – every iterator iteration would deep-copy an ever-growing dict | One shared, lock-protected store; no copies |
| Staleness – downstream component could read a stale snapshot | Always reads latest value via `GlobalStore.get()` |
| Memory – thousands of edges × 64 KB ≈ GBs of RAM | Store holds a single copy of each key |
| Checkpoint size – context blobs inflate run-store | GlobalStore dumped once per sub-job |

### BufferedStore recap (iterator components)

• Components tagged `iterator_boundary:true` receive a **BufferedStore** wrapper.  
• All `set()` calls cache in memory; `flush()` writes one summary per key to
  GlobalStore at loop end → near-zero lock contention.
  
  
  
  14 Addendum: Log File Support via CLI (v1.1)
Overview
Starting with v1.1, DataPY introduces support for structured JSON logging to a user-specified log file, passed via the CLI. This complements the existing observability features and ensures complete traceability of engine operations.

Log File Enablement
The CLI accepts a new global option:
--log-file /path/to/job_run.log

This option is required in production; log output to stdout is disabled unless explicitly enabled for debugging.

The path must be writeable; parent directories are auto-created.

Logging Behavior
A context-aware, thread-safe logger is initialized at job start and reused throughout the engine lifecycle.

Logs are written in structured JSON format with one line per event.

The log schema is stable and compatible with SIEM ingestion pipelines. Fields include:

timestamp (UTC, ISO format)

level (INFO, ERROR, etc.)

message (event type or summary)

job_id, run_id, subjob_id

component (if applicable)

error_type, row_count, duration_ms (if applicable)

No application state or global variables are ever logged directly—only metadata and execution events.

Logged Events
The following are guaranteed to emit JSON log entries:

Event	Example Message
Job started	"Job started"
Subjob started	"Subjob started"
Component executed	"Component execution"
GlobalStore set()	"GLOBAL_SET"
Checkpoint written	"Checkpoint committed"
Resume initiated	"Resume loaded"
Component failed	"Component failed"
Subjob skipped or retried	"Subjob retry initiated"
Final status	"Job completed" / "Job failed"

Design Notes
The logger is initialized in pype.core.observability.logging.setup_job_logger(log_path, context_dict) and reused throughout the engine.

Internally, all log calls are passed through a ContextualLogger wrapper that automatically includes job/run/subjob context on every entry.

The log file is shared across threads; threading.RLock protects I/O, but the file is flushed frequently to minimize buffer loss on failure.

No log rotation is implemented in v1.1. Rotation/purge policies are to be managed externally or added in a future release.

CLI enforcement of --log-file is handled by the top-level run.py, build.py, etc.




------addedum

15. forEach Iteration Model (v1.1)
forEach Component Specification
pythonclass ForEachComponent(BaseComponent):
    COMPONENT_NAME = "forEach"
    CATEGORY = "control"
    INPUT_PORTS = ["main"]           # DataFrame with items to iterate
    OUTPUT_PORTS = ["item"]          # Current item as DataFrame
    OUTPUT_GLOBALS = ["current_item", "current_index", "total_items", "ok "]
Iterator Boundary Processing

Subjob Dominance: forEach components force all downstream components into same subjob, overriding normal control edge subjob boundary rules
Recursive Execution: forEach components execute in nested loops, outer iterator processes each item completely (including all inner iterator cycles) before advancing
Data Flow: Each iterator iteration receives fresh input data; inner iterators process data from their immediate outer scope
State Management: Iterator components use BufferedStore during execution; GlobalStore updates only occur at iterator completion boundaries
Nesting Depth: No limit on iterator nesting depth; engine processes recursively following DAG structure

Iterator Metadata (Planner Stage)

Iterator Hierarchy: Planner generates execution tree showing iterator nesting relationships with enhanced metadata:
pythoniterator_metadata = {
    "forEach_files": {
        "iterator_depth": 0,
        "outer_iterator": None,
        "iteration_scope": ["process_file", "validate_file", "save_file"],
        "nested_iterators": ["forEach_orders"],
        "completion_targets": ["final_cleanup"]
    }
}

Loop Entry/Exit Points: Components marked with iterator depth and boundary positions
Execution Waves: Iterator-aware execution waves account for loop boundaries and data dependencies
Subjob Integration: Iterator metadata integrated into subjob metadata for unified execution planning

Iterator Control Flow Rules

Allowed Control Edges: Components in forEach scope can emit ok, error, if edges (stay within same subjob)
Forbidden Control Edges: Components in forEach scope cannot emit subjob_ok, subjob_error edges (validation error)
Loop Termination: forEach components control their own termination via data exhaustion or explicit stop conditions
Error Handling: Errors within iterator boundaries handled per iteration; retry occurs within iteration context
Checkpointing: Iterator state captured only at forEach completion boundaries; partial loop state not preserved

BufferedStore Semantics (Enhanced)

Scope: One BufferedStore instance per iterator boundary per iteration
Accumulation: Multiple set() calls within single iteration accumulate in buffer
Flush Timing: Buffer flushed to GlobalStore only at iterator completion
Nested Buffers: Inner iterator BufferedStores flush to outer iterator context, not directly to GlobalStore
Memory Efficiency: Prevents GlobalStore lock contention during high-frequency iterations

16. Subjob Boundary Rules (Updated for forEach)
Standard Subjob Boundary Creation
yaml# These create subjob boundaries:
connections:
  control:
    - "component_A (subjob_ok) component_B"     # Creates boundary
    - "component_A (subjob_error) component_B"  # Creates boundary
    - "forEach_comp (ok ) final_cleanup" # Creates boundary
forEach Subjob Dominance Rules
yaml# forEach forces downstream components into same subjob:
components:
  - name: forEach_files
    type: forEach
  - name: process_file     # Same subjob as forEach_files
    type: transform
  - name: validate_file    # Same subjob as forEach_files
    type: validate

connections:
  control:
    # ✅ Allowed (same subjob):
    - "validate_file (ok) save_file"
    - "validate_file (error) log_error"
    - "process_file (if1): \"large_file\" special_handler"
    
    # ❌ Forbidden (validation error):
    - "validate_file (subjob_ok) next_component"
    - "process_file (subjob_error) error_handler"
    
    # ✅ Creates new subjob:
    - "forEach_files (ok ) final_cleanup"
Validation Rules
python# Planner validation checks:
def validate_forEach_subjob_rules(dag):
    """Validate forEach subjob dominance rules"""
    errors = []
    
    forEach_scopes = get_forEach_component_scopes(dag)
    
    for forEach_comp, scope_components in forEach_scopes.items():
        for comp in scope_components:
            if comp == forEach_comp:
                continue  # forEach itself can emit ok 
                
            for edge in dag.out_edges(comp, data=True):
                trigger = edge[2].get('trigger')
                if trigger in ['subjob_ok', 'subjob_error']:
                    errors.append(
                        f"Component '{comp}' in forEach '{forEach_comp}' scope "
                        f"cannot emit '{trigger}' edges. Use 'ok'/'error' instead."
                    )
    
    return errors
17. Enhanced Iterator Boundary Marking
Boundary Marking Metadata (Enhanced)
python# Enhanced component metadata:
dag.nodes[component]['iterator_boundary'] = "forEach_files"    # Primary boundary
dag.nodes[component]['iterator_depth'] = 2                     # Nesting level  
dag.nodes[component]['outer_iterator'] = "forEach_customers"   # Parent iterator
dag.nodes[component]['is_subjob_start'] = False               # Existing metadata
dag.nodes[component]['subjob_id'] = "subjob_1"                # Forced by forEach
Subjob Metadata Integration
python# Enhanced subjob metadata:
subjob_metadata = {
    'subjob_1': {
        'components': ['forEach_customers', 'get_orders', 'forEach_orders', 'process_order'],
        'execution_order': [...],
        'has_iterators': True,                    # Iterator detection flag
        'iterator_components': {                  # Iterator hierarchy
            'forEach_customers': {
                'iterator_depth': 0,
                'iteration_scope': ['get_orders', 'forEach_orders', 'process_order'],
                'nested_iterators': ['forEach_orders'],
                'completion_targets': ['final_summary']
            },
            'forEach_orders': {
                'iterator_depth': 1,
                'parent_iterator': 'forEach_customers',
                'iteration_scope': ['process_order', 'save_order'],
                'nested_iterators': [],
                'completion_targets': []
            }
        }
    }
}
18. Engine Execution Strategy (Updated)
Orchestrator (Level 1) - Enhanced for forEach:

Asyncio-based subjob coordination with forEach-aware execution
Parallel subjob execution using asyncio.gather for non-forEach subjobs
forEach Execution: Recursive iteration handling for nested forEach components
Checkpointing at subjob boundaries (forEach completion boundaries)
Control flow and error handling with forEach-specific semantics
One SQLite DB per job for run store etc

ExecutionManager (Level 2) - forEach Integration:

Routes components based on job execution mode
BufferedStore Management: Injects BufferedStore for components in forEach scope
Iterator State Tracking: Maintains iteration state across nested forEach loops
Handles disk component internal conversions
Direct DataFrame passing (no wrapper overhead)

forEach Execution Flow:
python# Simplified forEach execution model:
async def execute_forEach_subjob(self, subjob_id, iterator_metadata):
    """Execute subjob containing forEach components with recursive nesting"""
    
    root_forEach = find_root_forEach_component(iterator_metadata)
    
    # Execute nested forEach recursively
    await self._execute_nested_forEach(root_forEach, iterator_metadata)

async def _execute_nested_forEach(self, forEach_comp, iterator_metadata):
    """Recursive execution of nested forEach components"""
    
    iteration_count = 0
    while True:
        # Execute forEach component to get next item
        result = await self.execute_component(forEach_comp)
        if result["item"] is None:
            break  # Iteration complete
            
        # Execute iteration scope with BufferedStore
        scope_components = iterator_metadata[forEach_comp]['iteration_scope']
        await self._execute_iteration_scope(scope_components, result)
        
        # Handle nested forEach components
        nested_forEach = iterator_metadata[forEach_comp]['nested_iterators']
        for nested_comp in nested_forEach:
            await self._execute_nested_forEach(nested_comp, iterator_metadata)
        
        iteration_count += 1




9  Event-Driven Scheduler & SubJobTracker (v1.1 Addendum)
This addendum replaces the former “execution-wave” planner logic with a deterministic, Talend-compatible token-based scheduler that maximises parallelism while preserving subjob_* semantics.

19.1  Planning-time – Dependency-Token Map
For every control edge the planner now emits a token instead of pre-grouping sub-jobs into waves.

Trigger type in YAML	Token generated	Meaning at runtime
ok, error, if…	<SourceComponentID>	Fires when that component finishes (success/fail/cond).
subjob_ok	SUBJOB_OK::<SourceSubJobID>	Fires when all tasks in the source sub-job succeed.
subjob_error	SUBJOB_ERR::<SourceSubJobID>	Fires on first failure inside the source sub-job.

Planner output (dag.msgpack) gains two new top-level keys:

python
Copy
Edit
"dependency_tokens": {                 # {targetSubJob: set(tokens)}
    "subjob_2": {"b", "SUBJOB_OK::subjob_0"},
    ...
}
"subjob_members": {                    # {subjobID: [componentIDs]}
    "subjob_0": ["a", "b", "c"],
    ...
}
19.2  Runtime – Ready-Queue Algorithm
text
Copy
Edit
ready_q        ← deque(sub-jobs with no waiting tokens)
running_q      ← set()
completed_comp ← set()

while ready_q or running_q:
    ▸ launch every sub-job in ready_q (asyncio.create_task) ➜ running_q
    ▸ await (subjobID, componentID, success) from event bus
        • Mark component done; completed_comp.add(componentID)
        • Clear matching tokens from downstream waiting_sets
        • Update SubJobTracker (see 19.3)
        • If tracker fires SUBJOB_OK / SUBJOB_ERR, clear those tokens too
        • Any sub-job whose waiting_set is now ∅ → ready_q.append()
        • When last component of a sub-job reports → running_q.discard()
Blocking pandas / disk I-O continues to run under loop.run_in_executor(pool, …).

19.3  SubJobTracker (one per sub-job)
python
Copy
Edit
@dataclass
class SubJobTracker:
    total_tasks: int
    succeeded_tasks: int = 0
    failed: bool = False
    ok_fired: bool = False
    err_fired: bool = False

    def notify(self, success: bool):
        self.succeeded_tasks += success
        self.failed |= not success

    def fire_ok(self)  -> bool: return (not self.failed
                                        and self.succeeded_tasks == self.total_tasks
                                        and not self.ok_fired)
    def fire_err(self) -> bool: return (self.failed and not self.err_fired)
Checkpoint & structured log entries are written the first time fire_ok() or fire_err() returns True.

19.4  Behaviour Reference Matrix
Scenario	When does downstream sub-job unblock?	Parallelism impact
b (ok) e_subjob	Immediately after b succeeds	e_subjob may start while other tasks in b’s sub-job run.
a (subjob_ok) d_subjob	After all components in a’s sub-job succeed and tracker fires SUBJOB_OK	Guarantees ready globals & checkpoint consistency.
x (subjob_error) y_subjob	On first failure inside x’s sub-job	Matches Talend OnSubjobError.

