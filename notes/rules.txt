DataPY Control-Flow & Sub-job Specification Rulebook
(Talend-compatible execution semantics — v 1.0, 6 Jul 2025)

Section 1 Fundamental Principles
1.1 Execution Model

A Job is an acyclic directed graph whose vertices are components and whose edges are either data (row-carrying) or control (trigger) links.

The engine walks the graph in topological order, but control edges can delay or accelerate downstream execution according to the rules in § 2.

Every component instance owns an independent Python worker (async task) that can be awaited, ensuring Talend-style thread-level isolation while remaining single-process friendly for local runs.

Determinism guarantee: given identical inputs, context, and component code, a job run produces identical data and control events (order + values).

1.2 Component State Machine

nginx
Copy
Edit
WAIT  ──► RUNNING ──► { OK | ERROR | SKIPPED }
                   ▲                  │
                   └──── cascade ─────┘
WAIT – upstream dependencies not yet satisfied.

RUNNING – execute() called, may emit rows/globals.

OK – completed without uncaught exception and die_on_error=False.

ERROR – uncaught exception or component sets die_on_error=True and raises.

SKIPPED – engine marked node inactive via cascade-skip (§ 6.3).

1.3 Timing & Ordering Rules

A component fires all of its control edges after its execute() coroutine returns and final state is known.

Control edges never interleave: ok/error/if links are enqueued synchronously before any downstream node starts.

Global variables become visible immediately after the firing component finishes execute() (they are committed atomically with the control edges).

1.4:  Dual Plane Execution – The engine has two orthogonal schedulers:

Data plane pushes PipelineData objects along data edges (§ 10).

Control plane triggers nodes via control edges (§ 2).
A component becomes runnable only when all inbound data ports are populated and at least one inbound control edge fires, unless the component is STARTABLE (§ 13.1).

Section 2 Control-Edge Specification
2.1 Edge Catalogue
Edge	Fires when	Fan-out allowed	Makes new sub-job?
ok	state = OK	1..∞	❌
error	state = ERROR	1..∞	❌
if	After state settles; condition == True	0..∞	❌
parallelise	Immediately after state = OK	≥2 (must list ≥2 targets)	✅ (one per target)
synchronise	All upstream parallel branches finish (OK or SKIPPED)	1..∞	❌
subjob_ok	Whole sub-job finishes with no ERROR	1..∞	❌ (edge is outside sub-job)
subjob_error	First ERROR inside sub-job	1..∞	❌

Add after table 2.1:

A data edge never causes a downstream node to start; only control edges (or STARTABLE flag) do.

parallelise marks its targets as the first node of a new sub-job ( = sub-job root).

2.1.1 ok Edge
Always fires once per component when state = OK.

If both ok and error edges exist, exactly one of them fires (mutually exclusive).
help.talend.com

2.1.2 error Edge
Fires once when state = ERROR.

Downstream nodes start even if Die on error flag is true, preserving Talend’s “allow custom recovery” pattern.
stackoverflow.com

2.1.3 if Edge
Syntax: (<order>): "<JEXL-like expr>". Lower order numbers evaluate first.

All conditions that evaluate True fire, not just the first.

Expressions run inside a restricted Jinja-safe evaluator with access to:

global, context, local after-variables.

Missing globals → expression returns False; evaluation error → component goes ERROR.
help.talend.com

2.1.4 parallelise & synchronise
parallelise forks N new sub-jobs, one per listed target; each child inherits a copy-on-write context.

synchronise waits for all listed upstream branches to reach OK or SKIPPED. If any branch ERRORs, the waiting component is cascade-skipped unless it also has an error edge.
stackoverflow.com
help.qlik.com

2.1.5 subjob_ok / subjob_error
Fired by the engine, not by a component, when a sub-job’s aggregate state becomes final.

Aggregate state is ERROR if any component is ERROR and no internal error-edge handled it. Otherwise OK (components may be SKIPPED).
community.qlik.com

Section 3 Sub-job Lifecycle Management
3.1 Boundary Detection Rules
A sub-job is a maximal connected region of the graph reachable via data links and control links excluding parallelise and synchronise. Any parallelise edge starts a new sub-job for its target; a synchronise edge joins but does not merge sub-jobs.

3.2 State Transitions

perl
Copy
Edit
PENDING ──► RUNNING ──► { OK | ERROR }
           ▲   ▲
           │   └─ finished when last component settles
           └─ starts when first component starts
3.3 Nested Sub-jobs
Nested spawning via chained parallelise edges is allowed. The outer sub-job’s state ignores inner failures once an error edge inside captures the error.

3.4 Orphan Components
A component with no incoming control edge inherits its parent sub-job. If unreachable from any startable node, planner error “UNCONNECTED_NODE”.

Section 4 Parallel Execution Rules
4.1 Fork/Join Semantics

Default engine backend uses asyncio.gather to mimic Talend’s multi-thread.

Each parallel branch receives an isolated copy of global map; write-back at join uses “last-writer-wins” with timestamp tie-break to preserve Talend’s undefined but deterministic JVM order.
community.qlik.com

4.2 Error Handling During Parallelism

If one branch ERRORs and no internal error edge handles it, its sub-job state = ERROR; siblings continue unless they access the shared checkpoint after failure.

synchronise node:

If all branches OK/SKIPPED → state = OK.

If any branch ERROR → synchronise becomes SKIPPED; follow cascade-skip (§ 6).

4.3 Multiple Synchronise
Allowed; each synchronise listens to a (branch-set, id) tuple; planner ensures no ambiguous joins.

Section 5 Global Variable Management
Phase	Visibility	Mutability
Inside component code	global is read/write dict, initially inherited from parent sub-job	✅
After component return	All writes staged; committed atomically	immutable snapshot
Across sub-jobs	Read-only unless explicit export via global_export list in component metadata	see §5.4

5.1 Lifecycle
Persists for entire job unless overwritten; destroyed on job completion or fatal abort.

5.2 Conflict Resolution
Parallel branches writing same key → newest commit_ts wins (monotonic clock).

5.3 Scope Rules

Globals created in a sub-job are visible to downstream sub-jobs after the firing of subjob_ok/subjob_error.

Globals are serialised into checkpoint files (§ 7) to guarantee resume reproducibility.

5.4 Modifying Existing Keys
Allowed; audit log records old/new value and writer component.

5.5 Joblet scope – When a joblet is expanded the loader rewrites every global write G inside it to caller.inner__G. Reads use the same prefixed key. Parent components therefore see joblet globals exactly as any other downstream global.



Section 6 Error Propagation Matrix
Upstream state	Downstream edge fires	Downstream default state if no edge
OK	ok, if (true), parallelise	WAIT
ERROR	error, subjob_error	SKIPPED
SKIPPED	none	SKIPPED

6.1 Cascade-Skip Algorithm
When a node reaches SKIPPED, all purely-dependent descendants (no alternate inbound edge) are recursively SKIPPED.

6.2 Component vs Sub-job Failure

Component ERROR without error edge → propagates to sub-job ERROR.

Sub-job ERROR → subjob_error edges fire; parent job keeps running unless aborted by policy.

Section 7 Checkpoint / Resume Specification
7.1 Persistence Granularity

Engine serialises at the end of every sub-job:

Global map snapshot

Per-component output hashes (for idempotency)

Context variables

7.2 Resume Rules

Detect last checkpoint with status = OK.

Mark all sub-jobs before it as SKIPPED, the checkpointed one as PENDING, later ones untouched.

On restart, re-hydrate globals & context from snapshot.

7.3 Parallel Resume
Branches that finished OK are not re-run; unfinished branches restart from fork point.
help.talend.com
help.qlik.com

Section 8 Edge-Case Handling
Circular control links → planner error “CYCLE_DETECTED”.

Infinite IF loops (condition with side-effects) → engine protection: max 100 re-evaluations → job ERROR “CONDITION_REEVALUATION_LIMIT”.

Resource exhaustion (thread pool, memory) → affected component ERROR; follows § 6 propagation.

Malformed YAML → loader error with JSON-schema path.

Forced Kill (SIGTERM) → engine attempts graceful checkpoint; if not possible within 30 s, abort without checkpoint.

Section 9 Validation & Test Strategy
Rule	Test Type	Observable Evidence
ok/error exclusivity	unit	Downstream firing list contains exactly one edge
Parallel join waits	integration	Log timestamp: join starts ≥ latest branch end
Global conflict resolution	unit (simulated ts)	Final value equals highest ts writer
Checkpoint resume	integration	Second run executes only sub-jobs ≥ failure point
Cascade-skip	unit	All descendants state = SKIPPED

Metrics to track: component runtimes, edge-fire counts, global-writes per scope, checkpoint size, time-to-resume.


Section 10 Data-Flow Semantics (NEW)
10.1 When Does a Data Edge Fire?
The sender queues each output port as soon as its component reaches OK.

The queue entry is a PipelineData reference; large frames may live in shared memory or disk spill (engine implementer’s choice, opaque to components).

10.2 Multiple Data Inputs
A component with allow_multi_in = False must have exactly one concrete inbound port per declared input_* placeholder. Violation → planner error.

With allow_multi_in = True, the engine accumulates arrivals from matching ports into inputs[port_name] = [PipelineData …]. The downstream component decides whether to concatenate or process incrementally.

10.3 Data vs Control Coordination
Data can physically arrive before its corresponding control edge fires, but the Task Scheduler MUST block the downstream coroutine until all expected inputs are present AND at least one inbound control edge has fired (or node is STARTABLE).

This guarantees Talend-compatible “don’t start until green light + full payload ready” behaviour.

10.4 Back-pressure Policy
Default local runner uses an unbounded async queue per edge. An enterprise runner SHOULD add a configurable max in-flight bytes or rows; when exceeded, the sender awaits consumer dequeue to mimic Talend’s blocking component connectors.

10.5 Wild-card Port Resolution
Loader inspects output_* or input_* entries in registry row.

Any connection whose concrete port name starts with that prefix is accepted and materialised.

Ports are sorted alphanumerically so that deterministic hashes can be calculated for checkpoints.

Example

yaml
Copy
Edit
connections:
  data:
    fanout.output_0 -> split.input_orders
    fanout.output_1 -> split.input_inventory
fanout advertises output_*; planner records two concrete ports.

Section 11 Joblet Integration Rules (NEW)
11.1 Expansion & Naming
Loader’s JobletExpander replaces the single caller node with the joblet’s internal DAG.

Every internal component is renamed callerName.__<internal> (double underscore separates).

Global variables likewise become callerName.__<internal>__GLOBAL.

11.2 Sub-job Boundaries
Each joblet invocation defines a new sub-job root.

The expanded nodes inherit the caller’s incoming control edges; outgoing control/data edges of the joblet are re-wired from its declared exposed ports to the parent graph.

11.3 Nested Joblets
Allowed to arbitrary depth. Loader keeps a recursion stack to prevent infinite expansion; >20 levels → loader error JOBLET_RECURSION_DEPTH.

11.4 Error Propagation
If an internal component fails and its error edge leads to a handler that also fails, the joblet’s sub-job state becomes ERROR and the parent graph’s subjob_error edge(s) fire.

Joblets may use internal subjob_ok / subjob_error edges within their own DAG; these do not percolate to the parent – they’re confined to the joblet’s sub-graph.

11.5 Data & Control Port Mapping
A joblet YAML must declare inputs: and outputs: sections listing the concrete port names the parent can connect to.

During expansion the expander injects proxy nodes that relay data/control between parent and internal root/leaf nodes. This keeps the flat DAG acyclic.

Example

yaml
Copy
Edit
# main job
components:
  - name: normalise_addr
    type: "@normalize_address"
    params: {}
connections:
  data:
    extract.main -> normalise_addr.input_customers
    normalise_addr.output_clean -> load_wh.input
Loader creates normalise_addr.__proxy_in → internal graph → normalise_addr.__proxy_out.

Section 12 Iterator Component Specification (NEW)
12.1 Purpose
iterator turns single incoming collection into N iterations, each as its own sub-job.

12.2 Sub-job per Iteration
For each element (row / group / file) the engine clones the iterator’s body DAG and schedules it as a child sub-job.

Sub-job name = iteratorName[#i].

12.3 Control Edges
Edge	Fires when
iteration_ok	iteration sub-job state = OK
iteration_error	iteration sub-job state = ERROR
iterator_complete	after all iterations finish (regardless of success)

12.4 Global Variable Modes (accumulate, reset, override)
accumulate (default): iterator merges each iteration’s globals into parent dict; list/-set merge for duplicate keys.

reset: globals cleared before each iteration; only last iteration’s globals survive.

override: standard last-writer-wins.

Mode is configured in iterator component metadata.

12.5 Empty Input Handling
Zero elements → iterator_complete fires immediately, no iteration_ok/error edges. State = OK.

12.6 Failure Strategy
fail_strategy in job_config controls:

halt (default): first iteration_error aborts remaining pending iterations; iterator state = ERROR.

continue: record failed iterations but continue; iterator_complete state = ERROR if any iteration failed.

12.7 Checkpoint & Resume
Each finished iteration is checkpointed; on resume, engine skips completed ones and restarts the earliest failed iteration.

Iterator State Diagram

nginx
Copy
Edit
IDLE ──► DISPATCHING ──► WAIT_ITER │
   ▲            │                 ▼
   └─ iterator_complete ◄─── ITER_DONE
Section 13 Component Startup & Execution Order (NEW)
13.1 STARTABLE Detection
Registry column startable = 1 marks a component that can begin without inbound control.

In addition, the planner designates a component STARTABLE if it has no inbound control edges after joblet expansion.

13.2 Multiple Startup Components
All STARTABLE nodes are placed in an initial ready queue and may run concurrently, subject to data-input readiness.

13.3 Ordering Guarantees
Scheduler is purely topological; there are no priority weights.

When multiple nodes become runnable simultaneously, engine may start them in any order but MUST respect data-dependency readiness (never starving one branch).

13.4 Context vs Global Variables
Aspect	Context	Global
Scope	process-wide, read-only snapshot of CLI-level variables (env, run-id)	dynamic, mutable during run
Lifetime	constant for entire job	until job end or explicit delete
Mutation	components may read but must not modify	components may read/write

Section 14 Error Propagation Matrix (extended)
Add iterator-specific rows:

Upstream state	Edge	Downstream default
iteration_error	iteration_error	parent iterator = ERROR
iterator_complete (some iteration_error)	iterator_complete	state = ERROR if fail_strategy=halt else OK

Section 15 Expanded Validation & Test Matrix (NEW/extended)
Scenario	Test Type	Expected Evidence
Wild-card port expansion	unit	Planner substitutes concrete ports, DAG still acyclic
Joblet nested globals	integration	caller.__inner__G observed downstream
Iterator empty input	unit	iterator_complete fires, zero iteration_ok
Resume after iteration 3 fail of 10	integration	second run executes iterations 3-10 only
STARTABLE with no inbound control	unit	node enters ready queue at T0
Back-pressure	stress	sender awaits when queue length > limit

Concrete Examples & Snippets
Example A Data vs Control Timing
scss
Copy
Edit
extract.main -> transform.input
extract (ok) transform
extract finishes OK → emits dataframe on main and ok control edge.

transform sees data ready and control fired → RUNNING.

If data arrived earlier (e.g. streaming mode) but extract not yet OK, scheduler blocks transform.

Example B Joblet Error Bubble
less
Copy
Edit
@address_cleaner
   └─ inner_map  --(error)--> inner_alert
inner_map fails, inner_alert also fails.
Result: joblet sub-job ERROR; parent subjob_error fires; global address_cleaner.__inner_map__ROW_ERR available to parent handlers.

Appendix A Safe Expression Grammar
Allowed	Example
Literals	42, 'prod', 3.14
Comparison	row_count > 0
Boolean ops	(status == 'OK') and (attempts < 3)
Arithmetic	(price * qty) > 1000
Functions (whitelisted)	len(rows), abs(delta)

Disallowed: imports, attribute access to __, mutation. Evaluation timeout default = 100 ms → if exceeded component ERROR.





Completion Criteria
A DataPY job is Talend-compatible when every test in the above matrix passes against a reference Talend job of identical topology, and the recorded event timeline (component state changes, edge firings, global writes) is isomorphic up to allowed timestamp skew.


Addendum: Rulebook v1.1 — Clarifications & Amendments (6 Jul 2025)
This section records accepted updates to Rulebook v1.1 based on post-publication analysis of Talend-compatible runtime edge cases. These clarifications apply retroactively and will be incorporated into Rulebook v1.2.

A1. Joblet Error Handling – Unhandled Internal Failures
Amend §11.4:
If any internal component of a joblet reaches state = ERROR and no internal error edge handles it, or if the error edge handler also fails and lacks a downstream edge, the joblet sub-job’s aggregate state is set to ERROR.

This guarantees that silent failures inside a joblet always bubble to the parent job via subjob_error, preserving execution integrity.

A2. die_on_error Inheritance and Override Behavior
New §11.6 (Joblet Error Policy):
die_on_error is respected only at the component level. If a component inside a joblet has die_on_error = False and no error edge, the component is marked ERROR, but the joblet continues if downstream dependencies do not require that output.

The joblet planner must therefore evaluate:

If the failure is unhandled → propagate to sub-job = ERROR

If failure is handled or allowed → sub-job may complete as OK or SKIPPED, based on cascade rules

There is no inheritance of die_on_error from parent to joblet or vice versa.

A3. Control Edge Firing on SKIPPED Components
Amend §2.1 (Control Edge Semantics):
A component in state = SKIPPED fires no control edges, even if they are defined.

New Debugging Note:
The engine SHOULD log a trace event explaining each SKIPPED transition and its cause (e.g., cascade due to upstream failure), for traceability and UI visualization.

A4. Iterator Globals & Checkpoint Consistency
Amend §12.4 (Global Variable Modes):
In accumulate mode, the engine MUST apply global writes from an iteration only if that iteration sub-job reaches state = OK.

If the iteration fails (state = ERROR), any global writes from that run are discarded during checkpoint/merge phase to avoid partial contamination.

Resume must re-execute failed iterations from the last consistent state.

A5. Sub-job Global Merge – Logical Clock Conflict Prevention
Amend §5.2 (Conflict Resolution):
To guarantee determinism across replays and avoid wall-clock inconsistencies, each sub-job is assigned a logical write clock (subjob_index + event counter).

When resolving parallel global writes, the engine prefers the higher logical clock. In case of tie, lexicographic component name order is the tie-breaker.

This ensures a deterministic global state independent of OS clock or concurrency order.

A6. STARTABLE Components Without Inputs
Amend §13.1 (STARTABLE Detection):
A component is considered STARTABLE if:

It is marked startable = 1 in registry, OR

It has no inbound control edge after joblet expansion

Clarification:
If a STARTABLE component also has no inbound data edges, it is scheduled immediately at T=0 as a root node, without delay.







End of Rulebook v 1.1